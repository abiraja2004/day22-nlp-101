{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Type `export PYTHONIOENCODING=utf8` in your terminal shell prior to running this notebook.\n",
    "\n",
    "Alternative:\n",
    "```python\n",
    "import sys  \n",
    "reload(sys)  \n",
    "sys.setdefaultencoding('utf8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data:\n",
    "\n",
    "## The 20 Newsgroups dataset\n",
    "\n",
    "* [Official Website](http://qwone.com/~jason/20Newsgroups/)\n",
    "* The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "* The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "## The 20 Newsgroups dataset\n",
    "\n",
    "* In the following we will use the built-in dataset loader for 20 newsgroups from scikit-learn.\n",
    "* In order to get faster execution times, we will work on a partial dataset with only 4 categories out of the 20 available in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 20 Newsgroups dataset\n",
    "\n",
    "The returned dataset is a scikit-learn “bunch”:\n",
    "\n",
    "* a simple holder object with fields that can be both accessed as python dict keys or object attributes for convenience\n",
    "* for instance, the *target_names* holds the list of the requested category names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "* Tokenization\n",
    "* Stop-word Removal\n",
    "* Stemming\n",
    "* Word Cloud\n",
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Intuition\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Datasets\n",
    "\n",
    "It would be desiarable to split the dataset into the following parts:\n",
    "\n",
    "* X_train\n",
    "* y_train\n",
    "* X_test\n",
    "* y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(twenty_train.data, twenty_train.target, train_size = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Datasets - X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Datasets - X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting list to Pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u\"From: paj@uk.co.gec-mrc (Paul Johnson)\\nSubject: Re: sore throat\\nReply-To: paj@uk.co.gec-mrc (Paul Johnson)\\nOrganization: GEC-Marconi Research Centre, Great Baddow, UK\\nLines: 29\\n\\nIn article <47835@sdcc12.ucsd.edu> wsun@jeeves.ucsd.edu (Fiberman) writes:\\n>I have had a sore throat for almost a week.  When I look into\\n>the mirror with the aid of a flash light, I see white plaques in\\n>the very back of my throat (on the sides).  I went to a health\\n>center to have a throat culture taken.  They said that I do not\\n>have strep throat.  Could a viral infection cause white plaques\\n>on the sides of my throat?\\n\\nFirst, I am not a doctor.  I know about this because I have been\\nthrough it.\\n\\nIt sounds like tonsilitis (lit. swollen tonsils).  Feel under your jaw\\nhinge for a swelling on each side.  If you find them, its tonsilitis.\\nI've had this a couple of times in the past.  The doctor prescribed a\\nweeks course of penicillin and that cleared it up.\\n\\nIn my case it was associated with glandular fever, which is a viral\\ninfection which (from my point of view) resembled flu and tonsilitis\\nthat kept coming back for a year or so.  There is a blood test for\\nthis.\\n\\nIn conclusion, see a doctor (if you have not done so already).\\n\\nPaul.\\n-- \\nPaul Johnson (paj@gec-mrc.co.uk).\\t    | Tel: +44 245 73331 ext 3245\\n--------------------------------------------+----------------------------------\\nThese ideas and others like them can be had | GEC-Marconi Research is not\\nfor $0.02 each from any reputable idealist. | responsible for my opinions\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_train = pd.Series(X_train)\n",
    "X_test = pd.Series(X_test)\n",
    "\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Datasets - y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Datasets - y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization breaks unstructured data, text, into chunks of\n",
    "information which can be counted as discrete elements. \n",
    "\n",
    "These counts of token\n",
    "occurrences in a document can be used directly as a vector representing that document.\n",
    "\n",
    "\n",
    "This immediately turns an unstructured string (text document) into a structured,\n",
    "numerical data structure suitable for machine learning.\n",
    "\n",
    "* Tokenization segments a document into its atomic elements (tokens)\n",
    "* Typically, our tokens are the words\n",
    "    - As an example where characters will be more appropriate as tokens, consider Language Detection\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above code will match any word characters until it reaches a non-word character, like a space\n",
    "* This can cause problems for words like *don’t* which will be read as two tokens - *don* and *t*.\n",
    "* A better tokeniser is TreeBankWordTokenizer which would break words like *don't* into *do* and *n't* \n",
    "* NLTK provides a number of pre-constructed tokenizers (like nltk.tokenize.simple)\n",
    "\n",
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     [from, :, paj, @, uk.co.gec-mrc, (, paul, john...\n",
       "1     [from, :, ed, @, cwis.unomaha.edu, (, ed, stas...\n",
       "2     [from, :, sloan, @, cis.uab.edu, (, kenneth, s...\n",
       "3     [from, :, jayne, @, mmalt.guild.org, (, jayne,...\n",
       "4     [from, :, hans, @, cs.kuleuven.ac.be, (, hans,...\n",
       "5     [from, :, geb, @, cs.pitt.edu, (, gordon, bank...\n",
       "6     [from, :, caralv, @, caralv.auto-trol.com, (, ...\n",
       "7     [from, :, donald, mackie, <, donald_mackie, @,...\n",
       "8     [from, :, jaeger, @, buphy.bu.edu, (, gregg, j...\n",
       "9     [from, :, draper, @, gnd1.wtp.gtefsd.com, (, p...\n",
       "10    [from, :, ricky, @, watson.ibm.com, (, rick, t...\n",
       "11    [from, :, bio1, @, navi.up.ac.za, (, fourie, j...\n",
       "12    [from, :, ata, @, hfsi.hfsi.com, (, john, ata,...\n",
       "13    [from, :, joshuaf, @, yang.earlham.edu, subjec...\n",
       "14    [from, :, dpw, @, sei.cmu.edu, (, david, wood,...\n",
       "15    [from, :, kaminski, @, netcom.com, (, peter, k...\n",
       "16    [from, :, g9134255, @, wampyr.cc.uow.edu.au, (...\n",
       "17    [from, :, geb, @, cs.pitt.edu, (, gordon, bank...\n",
       "18    [from, :, rgasch, @, nl.oracle.com, (, robert,...\n",
       "19    [from, :, camter28, @, astro.ocis.temple.edu, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_train.apply(lambda row: row.lower())\n",
    "X_train = X_train.apply(lambda row: tokenizer.tokenize(row))\n",
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop-word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the stopwords from the *stop_words* package, a [relatively conservative list](https://github.com/Alir3z4/stop-words/blob/master/english.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "print(en_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.apply(lambda row: [i for i in row if i not in en_stop])\n",
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "print(p_stemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Porter Stemmer\n",
    "\n",
    "* *p_stemmer* requires all tokens to be type str\n",
    "* p_stemmer returns the string parameter in stemmed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train = X_train.apply(lambda row: [p_stemmer.stem(i) for i in row])\n",
    "# X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = []\n",
    "for i in range(len(X_train)):\n",
    "    tokens = X_train[i]\n",
    "    tokens = [p_stemmer.stem(i) for i in tokens]\n",
    "    text = text + tokens\n",
    "    print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud\n",
    "\n",
    "WordClouds are a quick way to check the result of our preprocessing steps and debug them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "textall = \" \".join(text)\n",
    "textall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "wordcloud = WordCloud().generate(textall)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud with lower max font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=40).generate(textall)\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud with additional stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "haptik_mask = np.array(Image.open(\"./images/haptik.png\"))\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add(\"flight\")\n",
    "\n",
    "wc = WordCloud(max_words=2000, stopwords=stopwords)\n",
    "# generate word cloud\n",
    "wc = wc.generate(textall)\n",
    "\n",
    "# show\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing using `sklearn`\n",
    "\n",
    "`sklearn`'s `feature_extraction` module provides convenient API **CountVectorizer** to \"convert raw text into a matrix of token counts\" along with all the text processing steps we covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use TreeankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "vect.set_params(tokenizer=tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove English stop words\n",
    "vect.set_params(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "vect.set_params(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ignore terms that appear in more than 50% of the documents\n",
    "vect.set_params(max_df=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect.set_params(min_df=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** vect takes data as rows of text. Hence, we will have to get X_train in that format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(twenty_train.data, twenty_train.target, train_size = 0.8)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = pd.Series(X_train)\n",
    "X_test = pd.Series(X_test)\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learn the 'vocabulary' of the training data\n",
    "vect.fit(X_train)\n",
    "\n",
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we transform training data into a 'document-term matrix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simple_train_dtm = vect.transform(X_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine the vocabulary and document-term matrix together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(simple_train_dtm.toarray(), columns=vect.get_feature_names()).head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "nb.fit(simple_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make class predictions for test_dtm\n",
    "y_pred_class = nb.predict(test_dtm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:greyatom]",
   "language": "python",
   "name": "conda-env-greyatom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
