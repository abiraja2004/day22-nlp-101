{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. The challenge\n",
    "\n",
    "* The [`domain classification`](https://github.com/hellohaptik/haptik_open_datasets/tree/master/domain_classification) challenge from [Haptik Open Datasets](https://github.com/hellohaptik/haptik_open_datasets): Classify (short) user queries into one of the given nine classes.\n",
    "* Expected Performance\n",
    "    - Suggested metric: Overall Accuracy/Subset Accuracy\n",
    "    - Baseline Performance: 70%\n",
    "    - Expected Performance: >80%\n",
    "\n",
    "**Some observations**:\n",
    "\n",
    "1. Messages contain system specific content that needs to be removed.\n",
    "2. (Overall) Subset Accuracy is `not appropriate` for this task, so we'll use label accuracy for model development.\n",
    "    - For example, Subset Accuracy cannot distinguish between a model that gets 1 out of 3 positive labels wrong and a model that gets 2 out of 3 positive labels wrong (for a given document).\n",
    "\n",
    "## 1.1. (Multi-class) Multi-label Classification\n",
    "\n",
    "* **Multiclass classification** means a classification task with more than two classes\n",
    "* **Multilabel classification** assigns to each sample a set of target labels\n",
    "---\n",
    "* Inherently multiclass: Naive Bayes, LDA and QDA, Decision Trees, Random Forests, Nearest Neighbors, setting multi_class='multinomial' in sklearn.linear_model.LogisticRegression.\n",
    "* Support multilabel: Decision Trees, Random Forests, Nearest Neighbors.\n",
    "---\n",
    "\n",
    "* [One vs Rest (OvR) Classifiers](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html) are the bread and butter of multi-class and multi-label classification. Scikit-learn lets us turn any binary classifier into a multi-class/multi-label classifier using the *One vs Rest* (OvR) scheme.\n",
    "* We have to use [Multi-label Binarizer](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html) to get the data into the right format for training.\n",
    "* Scikit-learn has [a page on Multiclass and multilabel algorithms](http://scikit-learn.org/stable/modules/multiclass.html).\n",
    "* [scikit-multilearn](http://scikit.ml/) is a multi-label classification library for python, which you can check out if you are feeling adventurous.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from haptik import *\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit_transform([(1, 2), (3,)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlb.fit_transform([set(['sci-fi', 'thriller']), set(['comedy'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlb.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The Codebase\n",
    "\n",
    "We'll develop a class `Haptik` for organising our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Importing necessary modules and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import time                                                \n",
    "import pickle\n",
    "import copy\n",
    "import string\n",
    "import operator\n",
    "from collections import OrderedDict\n",
    "from autocorrect import spell\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decorators inside classes are not a good idea\n",
    "# https://stackoverflow.com/questions/13852138/how-can-i-define-decorator-method-inside-class\n",
    "def timeit(method):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = method(*args, **kw)\n",
    "        te = time.time()\n",
    "        if 'log_time' in kw:\n",
    "            name = kw.get('log_name', method.__name__.upper())\n",
    "            kw['log_time'][name] = int((te - ts) * 1000)\n",
    "        else:\n",
    "            print('%r  %2.2f ms' % \\\n",
    "                  (method.__name__, (te - ts) * 1000))\n",
    "        return result\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_dump(data, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def pickle_load(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # https://stackoverflow.com/questions/28218466/unpickling-a-python-2-object-with-python-3\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. `Haptik` Class: Constructor\n",
    "\n",
    "**Note**:\n",
    "* **multi_label_binarizer**: We don't need to use `MultiLabelBinarizer()` from scikit-learn for bringing the target vector (matrix) into the correct format since it's almost in the correct format already, so we write out own binarizer.\n",
    "\n",
    "Let's inspect our datasets and see if we need to further manipulate them into the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptik:\n",
    "    \n",
    "    def xy_separator(self, df):\n",
    "        \"\"\"Separates feature matrix (column 0) and target vector (the rest)\n",
    "        \"\"\"\n",
    "        X = df.iloc[:, 0]\n",
    "        y = df.iloc[:, 1:]\n",
    "        return X, y\n",
    "    \n",
    "    def multi_label_binarizer(self, df):\n",
    "        \"\"\"Maps [\"T\", \"F\"] to [1, 0] in a given dataframe\n",
    "        \"\"\"\n",
    "        df = df.astype(str).applymap(lambda x: 1 if x=='T' else 0)\n",
    "        return df\n",
    "\n",
    "    def __init__(self, path, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.path = path\n",
    "        self.train = pd.read_csv(path + '/train_data.csv', encoding='utf-8')\n",
    "        self.test = pd.read_csv(path + '/test_data.csv', encoding='utf-8')\n",
    "        self.X_train, self.y_train = self.xy_separator(self.train)\n",
    "        self.X_test, self.y_test = self.xy_separator(self.test)\n",
    "        self.target_names = self.y_train.columns\n",
    "        \n",
    "        self.y_train = self.multi_label_binarizer(self.y_train)\n",
    "        self.y_test = self.multi_label_binarizer(self.y_test)\n",
    "        \n",
    "        self.X_train = pd.Series(self.X_train)\n",
    "        self.X_test = pd.Series(self.X_test)\n",
    "        \n",
    "        # Compute distribution of label frequencies\n",
    "        freq = np.ravel(self.y_train.sum(axis=0))\n",
    "        freq = dict(zip(self.y_train.columns, freq))\n",
    "        freq = OrderedDict(sorted(freq.items(), key=operator.itemgetter(1)))\n",
    "        self.yhist = pd.DataFrame({'label':freq.keys(), 'count':freq.values()})\n",
    "        self.yhist['normalized'] = self.yhist['count']/self.yhist['count'].sum()\n",
    "        \n",
    "        self.gridsearchcv = None\n",
    "        self.model = None\n",
    "        self.train_dtm = None\n",
    "        self.test_dtm = None\n",
    "        self.feature_names = None\n",
    "        self.feature_selector = None\n",
    "        \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Haptik('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the target matrix is in the format needed by multi-label classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. `Haptik` Class: String Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptik(Haptik):\n",
    "    \n",
    "    def summary(self):\n",
    "        \"\"\"A chainable wrapper for dunder repr\n",
    "        \"\"\"\n",
    "        print(self.__repr__())\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"Defines String representation of the class\n",
    "        \"\"\"\n",
    "        output = '\\nX_train shape: ' + str(self.X_train.shape) + '\\ny_train shape: ' + str(self.y_train.shape) + \\\n",
    "                 '\\nX_test shape: ' + str(self.X_test.shape) + '\\ny_test shape: ' + str(self.y_test.shape) + '\\n'\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. `Haptik` Class: Document Preprocessing\n",
    "\n",
    "Note:\n",
    "* The list *sw_curated* contains our curated list of stop words which we'll remove from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptik(Haptik):\n",
    "    \n",
    "    def _preprocess(self, listlikeobj, stop_lists=None):\n",
    "        \"\"\"Applies pre-processing pipelines to lists of string\n",
    "        \"\"\"\n",
    "        \n",
    "        numeric = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', \\\n",
    "                    'ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen', 'Sixteen', 'Seventeen', \\\n",
    "                    'Eighteen', 'Nineteen', 'Twenty', 'Twenty-one', 'Twenty-two', 'Twenty-three', \\\n",
    "                    'Twenty-four', 'Twenty-five', 'Twenty-six', 'Twenty-seven', 'Twenty-eight', \\\n",
    "                    'Twenty-nine', 'Thirty', 'Thirty-one']\n",
    "        \n",
    "        ordinal = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eight', 'ninth', \\\n",
    "                    'tenth', 'eleventh', 'twelfth', 'thirteenth', 'fourteenth', 'fifteenth', 'sixteenth', \\\n",
    "                    'seventeenth', 'eighteenth', 'nineteenth', 'twentieth', 'twenty-first', 'twenty-second', \\\n",
    "                    'twenty-third', 'twenty-fourth', 'twenty-fifth', \\\n",
    "                    'twenty-sixth', 'twenty-seventh', 'twenty eighth', 'twenty-ninth', 'thirtieth', 'thirty-first']\n",
    "        \n",
    "        \n",
    "        en_stop = get_stop_words('en')\n",
    "        tokenizer = TreebankWordTokenizer()\n",
    "        p_stemmer = PorterStemmer()\n",
    "        \n",
    "        listlikeobj = listlikeobj.apply(lambda row: row.lower())\n",
    "        listlikeobj = listlikeobj.apply(lambda row: tokenizer.tokenize(row))\n",
    "        listlikeobj = listlikeobj.apply(lambda row: [i for i in row if i not in en_stop])\n",
    "        listlikeobj = listlikeobj.apply(lambda row: [i for i in row if i not in string.punctuation])\n",
    "        listlikeobj = listlikeobj.apply(lambda row: [p_stemmer.stem(i) for i in row])\n",
    "        if stop_lists:\n",
    "            for sw_dict in stop_lists:\n",
    "                listlikeobj = listlikeobj.apply(lambda row: [i for i in row if i not in sw_dict])\n",
    "        #listlikeobj = listlikeobj.apply(lambda row: [re.sub(r'\\d', \"#\", i) for i in row])\n",
    "        #listlikeobj = listlikeobj.apply(lambda row: [\"#\" for i in row if i in numeric])\n",
    "        #listlikeobj = listlikeobj.apply(lambda row: [\"#th\" for i in row if i in ordinal])\n",
    "        #print(listlikeobj)\n",
    "        \n",
    "        #listlikeobj = listlikeobj.apply(lambda row: [spell(i) for i in row if len(i)>6])\n",
    "        \n",
    "        \n",
    "        return listlikeobj\n",
    "    \n",
    "    @timeit\n",
    "    def preprocess(self, stop_lists=None):\n",
    "        \"\"\"Apply pre-processing on training and testing documents\n",
    "        \"\"\"\n",
    "        self.X_train = self._preprocess(self.X_train, stop_lists)\n",
    "        self.X_test = self._preprocess(self.X_test, stop_lists)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @timeit\n",
    "    def vectorize(self, vectorizer=CountVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)):\n",
    "        \"\"\"Vectorize the train and test data\n",
    "        \"\"\"\n",
    "        X_train = pd.Series([' '.join(x) for x in self.X_train])\n",
    "        X_test = pd.Series([' '.join(x) for x in self.X_test])\n",
    "        \n",
    "        # Vectorize\n",
    "        vectorizer.fit(X_train)\n",
    "        self.train_dtm = vectorizer.transform(X_train)\n",
    "        self.test_dtm = vectorizer.transform(X_test)\n",
    "        self.feature_names = vectorizer.get_feature_names()\n",
    "        \n",
    "        # token frequency count\n",
    "        freq = np.ravel(self.train_dtm.sum(axis=0))\n",
    "        vocab = [v[0] for v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]\n",
    "        freq_sorted = dict(zip(vocab, freq))\n",
    "        freq_dict = OrderedDict(sorted(freq_sorted.items(), key=operator.itemgetter(1)))\n",
    "        self.wordfreq = pd.DataFrame({'word':freq_dict.keys(), 'count':freq_dict.values()})\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    @timeit\n",
    "    def reduce_dimensions(self, k=15000):\n",
    "        \"\"\"Reduce dimensionality\"\"\"\n",
    "        ch2 = SelectKBest(chi2, k=15000)\n",
    "        ch2.fit(self.train_dtm, self.y_train)\n",
    "        self.train_dtm = ch2.transform(self.train_dtm)\n",
    "        self.test_dtm = ch2.transform(self.test_dtm)\n",
    "\n",
    "        # keep selected feature names\n",
    "        self.feature_names = [self.feature_names[i] for i in ch2.get_support(indices=True)]\n",
    "        self.feature_selector = ch2\n",
    "        \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. `Haptik` Class: Visualizations\n",
    "\n",
    "We'll add some functions for the two most basic visualisations that are appropriate here:\n",
    "\n",
    "* Wordcloud\n",
    "* Histogram of distributions of classes/labels in the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note: since the dataset is comparatively large, creating a wordcloud will take longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptik(Haptik):\n",
    "    \n",
    "    @timeit\n",
    "    def wordcloud(self):\n",
    "        \"\"\"Create a word cloud\n",
    "        \"\"\"\n",
    "        text = []\n",
    "        for i in range(len(self.X_train)):\n",
    "            text = text + [i for i in self.X_train[i]]\n",
    "        textall = \" \".join(text)\n",
    "        wordcloud = WordCloud(max_font_size=40).generate(textall)\n",
    "        plt.figure()\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        return self\n",
    "    \n",
    "    @timeit\n",
    "    def yhist_plot(self):\n",
    "        \"\"\"Create a histogram (plot) of label distribution\n",
    "        \"\"\"\n",
    "        sns.set_style(\"whitegrid\")\n",
    "        ax = sns.barplot(x=\"label\", y=\"normalized\", data=self.yhist)\n",
    "        plt.show()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAEBCAYAAACe6Rn8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njCVARKRgLAEiIgX7f2mzrTq1J7q7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1278e5c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'yhist_plot'  395.74 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "X_train shape: (40659,)\n",
       "y_train shape: (40659, 9)\n",
       "X_test shape: (10000,)\n",
       "y_test shape: (10000, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Haptik('./data')\n",
    "x.yhist_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. `Haptik` Class: Modeling (the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptik(Haptik):\n",
    "    \n",
    "    def label_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"Compute label accuracy\n",
    "        \"\"\"\n",
    "        res = (y_true == y_pred)\n",
    "        return (res).sum().sum()/res.size\n",
    "    \n",
    "    @timeit\n",
    "    def classify(self,\n",
    "                 model=OneVsRestClassifier(MultinomialNB())):\n",
    "        \"\"\"Fit a model to the dataset\n",
    "        \"\"\"\n",
    "        # Clone local copies\n",
    "        X_train = copy.deepcopy(self.train_dtm)\n",
    "        X_test = copy.deepcopy(self.test_dtm)\n",
    "        y_train = copy.deepcopy(self.y_train)\n",
    "        y_test = copy.deepcopy(self.y_test)\n",
    "        \n",
    "\n",
    "        # Fit and predict\n",
    "        model.fit(X_train, y_train)\n",
    "        if isinstance(model, GridSearchCV): # assumes GridSearchCV() has been imported\n",
    "            self.gridsearchcv = copy.deepcopy(model)\n",
    "            model = copy.deepcopy(model.best_estimator_)\n",
    "            \n",
    "        self.y_pred_class = model.predict(X_test)\n",
    "        self.model = model\n",
    "        \n",
    "        # Compute metrics\n",
    "        self.accuracy_subset = metrics.accuracy_score(y_test, self.y_pred_class)\n",
    "        self.accuracy_label = self.label_accuracy(y_test, self.y_pred_class)\n",
    "        self.c_report = metrics.classification_report(y_test, self.y_pred_class)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def results(self):\n",
    "        \"\"\"Print accuracy and other metrics\n",
    "        \"\"\"\n",
    "        print('accuracy_label: ', self.accuracy_label)\n",
    "        print('accuracy_subset: ', self.accuracy_subset)\n",
    "        print('classification report: \\n', self.c_report)\n",
    "        return self\n",
    "    \n",
    "    def results_cv(self):\n",
    "        \"\"\"Print the best models and the details of cv rounds\n",
    "        \"\"\"\n",
    "        print('cv results: \\n', pd.DataFrame(self.gridsearchcv.cv_results_))\n",
    "        print('best parameters: \\n', self.gridsearchcv.best_params_)\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Haptik(Haptik):\n",
    "    \n",
    "    def cv_scores(self):\n",
    "        \"\"\"Plot mean scores for a particular grid object\n",
    "        \"\"\"\n",
    "        mean_test_scores = self.gridsearchcv.cv_results_['mean_test_score']\n",
    "        mean_train_scores = self.gridsearchcv.cv_results_['mean_train_score']\n",
    "        plt.figure(figsize=(10,6))\n",
    "\n",
    "        param_values =[str(x) for x in self.gridsearchcv.param_grid.values()[0]]\n",
    "        plt.plot(param_values, mean_train_scores, c='r')\n",
    "        plt.plot(param_values, mean_test_scores, c='g')\n",
    "        \n",
    "        plt.xlabel(self.gridsearchcv.param_grid.keys()[0])\n",
    "        plt.ylabel('mean scores')\n",
    "        plt.show()\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Haptik('./data').preprocess().vectorize()\n",
    "pickle_dump(x, '0.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Iteration 1: Default Values\n",
    "\n",
    "1. **model** = OneVsRestClassifier(MultinomialNB()), \n",
    "2. **vectorizer** = CountVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
    "3. **Label Accuracy** = 94.03%\n",
    "4. **Subset Accuracy** = 63.63%\n",
    "5. **Time Taken**\n",
    "    - `preprocess()`: 16042.35ms (16s)\n",
    "    - `classify()`: 2839.06ms (3s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.classify().results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.wordfreq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2. Iteration 2: Default Values\n",
    "\n",
    "1. **model** = OneVsRestClassifier(MultinomialNB()), \n",
    "2. **vectorizer** = TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
    "3. **Label Accuracy** = \n",
    "4. **Subset Accuracy** = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.vectorize(vectorizer=TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=4))\n",
    "x.classify().results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Iteration 3: Random Forest\n",
    "\n",
    "1. **model** =  \n",
    "2. **vectorizer** = TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
    "3. **Label Accuracy** = \n",
    "4. **Subset Accuracy** = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from haptik import Haptik, timeit, pickle_dump, pickle_load\n",
    "\n",
    "x = Haptik('./data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.reduce_dimensions(k=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.classify(model=RandomForestClassifier(n_estimators=200, max_depth=2500,\n",
    "                                        min_samples_split=5, n_jobs=-1,\n",
    "                                        max_features=0.008, # 0.00735215 = sqrt(18,500)/18,500\n",
    "                                        random_state=42, verbose=3)).results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Iteration 4: Random Forest Grid Search\n",
    "\n",
    "1. **model** =  \n",
    "2. **vectorizer** = TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
    "3. **Label Accuracy** = \n",
    "4. **Subset Accuracy** = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "    \n",
    "param_grid_rf = {\"rf__n_estimators\": [100, 150, 200, 250, 300, 350, 400],\n",
    "                  \"rf__max_depth\": [2000, 2250, 2500, 2750, 3000],\n",
    "                  \"rf__min_samples_split\": [4, 5, 6],\n",
    "                  \"rf__max_features\": [0.005, 0.006, 0.007, 0.00735215, 0.008, 0.07, 0.2]}\n",
    "\n",
    "# https://stackoverflow.com/questions/34889110/random-forest-with-gridsearchcv-error-on-param-grid\n",
    "def rf_search(param_grid):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    pipeline = Pipeline([\n",
    "        ('rf', rf)\n",
    "    ])\n",
    "    grid = GridSearchCV(pipeline, param_grid, n_jobs=-1, verbose=3, cv=3)\n",
    "    return grid\n",
    "\n",
    "x = Haptik('./data').preprocess().vectorize().reduce_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.classify(model=rf_search(param_grid_rf)).results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_dump(x, 'rf_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod1 = copy.deepcopy(x.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Iteration 5: Multinomial NB Grid Search\n",
    "\n",
    "1. **model** =  \n",
    "2. **vectorizer** = TfidfVectorizer(ngram_range=(1, 2), max_df=0.5, min_df=2)\n",
    "3. **Label Accuracy** = \n",
    "4. **Subset Accuracy** = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "param_grid = {\"estimator__alpha\": np.arange(0, 1, 0.05),\n",
    "              \"estimator__fit_prior\": [True, False]}\n",
    "\n",
    "def mnb_search(param_grid):\n",
    "    nb = OneVsRestClassifier(MultinomialNB())\n",
    "    grid = GridSearchCV(nb, param_grid, n_jobs=-1, verbose=3)\n",
    "    return grid\n",
    "\n",
    "x.classify(model=mnb_search(param_grid))\n",
    "pickle_dump(x, 'mnb_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod2 = copy.deepcopy(x.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exhaustive Hyperparameter Tuning\n",
    "\n",
    "**Warning**: Do not try this at home. Do it on the cloud. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid_1 = {\"n_estimators\": [300, 400, 500, 600, 700, 800]}\n",
    "param_grid_2 = {\"n_estimators\": [300],\n",
    "                \"max_depth\": [3, 4, 5, 6, 7, 8, 9]}\n",
    "param_grid_3 = {\"n_estimators\": [300],\n",
    "                \"max_depth\": [3, 7],\n",
    "                \"max_features\": [1, 3, 10]}\n",
    "param_grid_4 = {\"n_estimators\": [300],\n",
    "                \"max_depth\": [3, 7],\n",
    "                \"max_features\": [1, 3, 10],\n",
    "                \"min_samples_split\": [2, 4, 8]}\n",
    "param_grid_5 = {\"n_estimators\": [300],\n",
    "                \"max_depth\": [3, 7],\n",
    "                \"max_features\": [1, 3, 10],\n",
    "                \"min_samples_split\": [2, 4, 8],\n",
    "                \"min_samples_leaf\": [1, 3, 10]}\n",
    "\n",
    "# https://stackoverflow.com/questions/34889110/random-forest-with-gridsearchcv-error-on-param-grid\n",
    "def rf_search(param_grid):\n",
    "    rf = RandomForestClassifier()\n",
    "    grid = GridSearchCV(rf, param_grid, n_jobs=-1, verbose=3)\n",
    "    return grid\n",
    "\n",
    "x = Haptik('./data').preprocess()\n",
    "\n",
    "x.classify(model=rf_search(param_grid_1)).results()\n",
    "pickle_dump(x, 'rf_job_1.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Way Forward\n",
    "\n",
    "* Named Entity Recognition\n",
    "* Spell Check: People make many typing mistakes or shorten words while typing. Correcting them should help improve the accuracy.\n",
    "\n",
    "Here are some curated hint you can incorporate in your codebase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sw_curated = ['a9c0', 'f0af5ee2b89b', '676951', 'hi', '727893', '55616', 'task_nam',\n",
    "              'user_id', '50', '16', 'exotel','ok', 'haptik', 'nearbi',\n",
    "              'api_nam', 'offset', 'user_id', '00', 'pleas', 'can', 'pl',\n",
    "              'drink_water', 'reminer_list', 'trains_api', 'product_id', \n",
    "              'I', 'j', 'se', 'sl', '10', 'hai', 'll', 'kya', 'rs', 'sorri',\n",
    "              'know', '30', 'one', '10 00', 'bu', 'reminder_list',\n",
    "              'plz', 'everi']\n",
    "\n",
    "temporal = ['time', 'will', 'today', 'everyday', 'now', 'hour', 'tomorrow', 'day',\\\n",
    "'apr', 'april', 'fri', 'friday', 'june', 'later', 'may', 'min', 'minut',\\\n",
    "'mon', 'month', 'noon', 'nov', 'oct', 'octob', 'sat', 'sep', 'sept', 'sun',\\\n",
    "'sunday', 'thursday', 'tommorow', 'wed', 'wednesday', 'week', 'weekend', 'yesterday', 'hr']\n",
    "\n",
    "misc = ['alreadi', 'also', 'around', 'ca', 'do', 'done', 'dont', 'iam', 'im', 'in',\\\n",
    "'it', 'just', 'let', 'much', 'name', 'oh', 'ok', 'okay', 'shall', 'sure', 'th',\\\n",
    "'that', 'to', 'wat', 'wil', 'yet', 'near', 'abt', 'didnt', 'fr', 'kk', 'll', 'nd', 'ohk']\n",
    "\n",
    "pleasantries = ['thank', 'hello', 'hi', 'hii', 'thanku', 'thnk', 'thnx', 'thx']\n",
    "\n",
    "address = ['bro', 'dr', 'dude', 'mam', 'miss', 'mr', 'sir']\n",
    "\n",
    "hindi = ['abhi', 'bhai', 'ho', 'hu', 'ji', 'ka', 'ke', 'ki', 'ko']\n",
    "\n",
    "numeric = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', \\\n",
    "'ten', 'Eleven', 'Twelve', 'Thirteen', 'Fourteen', 'Fifteen', 'Sixteen', 'Seventeen', \\\n",
    "'Eighteen', 'Nineteen', 'Twenty', 'Twenty-one', 'Twenty-two', 'Twenty-three', \\\n",
    "'Twenty-four', 'Twenty-five', 'Twenty-six', 'Twenty-seven', 'Twenty-eight', \\\n",
    "'Twenty-nine', 'Thirty', 'Thirty-one']\n",
    "\n",
    "ordinal = ['first', 'second', 'third', 'fourth', 'fifth', 'sixth', 'seventh', 'eight', 'ninth', \\\n",
    "'tenth', 'eleventh', 'twelfth', 'thirteenth', 'fourteenth', 'fifteenth', 'sixteenth', \\\n",
    "'seventeenth', 'eighteenth', 'nineteenth', 'twentieth', 'twenty-first', 'twenty-second', \\\n",
    "'twenty-third', 'twenty-fourth', 'twenty-fifth', \\\n",
    "'twenty-sixth', 'twenty-seventh', 'twenty eighth', 'twenty-ninth', 'thirtieth', 'thirty-first']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                                    [7am]\n",
      "1                                           [chocol, cake]\n",
      "2              [close, mortic, tenon, joint, door, diment]\n",
      "3                                  [train, eppo, kelambum]\n",
      "4                                 [cancel, flight, ticket]\n",
      "5                                       [chamg, 12pm, 9pm]\n",
      "6                                    [want, go, rajasthan]\n",
      "7                                                   [room]\n",
      "8                                 [arrang, flight, ticket]\n",
      "9                                           [kind, remind]\n",
      "10                                 [jamshedpur, jharkhand]\n",
      "11                                     [noidaa, secot, 44]\n",
      "12                                      [flight, spicejet]\n",
      "13                                                  [uber]\n",
      "14                                                [3.3.17]\n",
      "15                                            [fare, high]\n",
      "16                             [train, run, jalgaon, pune]\n",
      "17                   [send, current, statu, train, ticket]\n",
      "18                                  [ye, 've, got, remind]\n",
      "19                                             [wake, 6am]\n",
      "20                                 [patli, aligarh, train]\n",
      "21                                  [look, flight, option]\n",
      "22                                     [thrursday, remind]\n",
      "23                                                [remind]\n",
      "24                             [payment, made, ola, money]\n",
      "25                                     [silchar, guwahati]\n",
      "26                                           [five, peopl]\n",
      "27       [like, cancel, wake, call, 05, march, 05:00, 7...\n",
      "28                [hey, find, nearest, medic, store, task]\n",
      "29                                  [detail, train, 12630]\n",
      "                               ...                        \n",
      "40629                                               [auto]\n",
      "40630                                        [what, updat]\n",
      "40631                                  [understand, hindi]\n",
      "40632                                   [locat, churchgat]\n",
      "40633                                                   []\n",
      "40634                                           [u, heart]\n",
      "40635                                          [u, welcom]\n",
      "40636                         [thank-you.., appreci, help]\n",
      "40637                                                  [4]\n",
      "40638                                             [anymor]\n",
      "40639                         [abob, offer, still, applic]\n",
      "40640                                       [what, salari]\n",
      "40641                                                [pmk]\n",
      "40642                                           [bareilli]\n",
      "40643                      [want, book, privat, jet, task]\n",
      "40644                     [coupon, code, shop, myntra.com]\n",
      "40645                              [get, review, gujarati]\n",
      "40646                                         [tat, great]\n",
      "40647                                            [thanks☺]\n",
      "40648                                [feel, bore, today..]\n",
      "40649                        [u, tell, call, girl, number]\n",
      "40650                                         [madem, tri]\n",
      "40651                       [ntng, els, sir.just, ask, 's]\n",
      "40652                                           [bad, guy]\n",
      "40653                                               [chkd]\n",
      "40654                            [add, money, expir, date]\n",
      "40655                                             [receip]\n",
      "40656                                              [insur]\n",
      "40657                                      [unabl, search]\n",
      "40658                                   ['s, good, experi]\n",
      "Name: message, Length: 40659, dtype: object\n",
      "0                               [nearest, metro, station]\n",
      "1                    [pick, n, drop, servic, trough, cab]\n",
      "2                                       [want, buy, bick]\n",
      "3                                           [show, pizza]\n",
      "4                    [cheapest, packag, andaman, nicobar]\n",
      "5                      [remind, take, money, mummi, 2:15]\n",
      "6                                      [yoga, centr, gym]\n",
      "7                             [cant, found, chines, menu]\n",
      "8                                    [help, book, ticket]\n",
      "9                                   [want, remov, remind]\n",
      "10                                    [xiaomi, redmi, 3s]\n",
      "11                                         [film, releas]\n",
      "12                                [hey, want, full, pair]\n",
      "13                  [ting, 1st, flight, frm, hyd, mumbai]\n",
      "14        [suggest, slogan, onlin, shop, compani, airsal]\n",
      "15      [need, book, flight, ticket, canada, yyz, airp...\n",
      "16      [u, gv, run, statu, kanchankanya, express, ......\n",
      "17                                 [whichev, seat, avail]\n",
      "18                    [give, remind, call, harshal, 5:25]\n",
      "19                [puneet, want, train, tanur, ernakulam]\n",
      "20                     [movi, list, 30th, septemb, right]\n",
      "21                       [stop, daili, morn, alarm, need]\n",
      "22      [tell, show, tim, saturday, cost, foroum, gopa...\n",
      "23                                [remind, meet, 7, even]\n",
      "24                                    [smokin, joe, open]\n",
      "25                                    [hey., food, arriv]\n",
      "26                                        [flight, delhi]\n",
      "27                             [ye, plan, trip, thailand]\n",
      "28                        [help, nearest, relianc, store]\n",
      "29                                   [get, remind, watwr]\n",
      "                              ...                        \n",
      "9970                                          [n't, want]\n",
      "9971                                                [thk]\n",
      "9972                                               [help]\n",
      "9973                                                [atm]\n",
      "9974                          [hey, exam, gon, na, start]\n",
      "9975                                         [happeningg]\n",
      "9976                                         [ahamadabad]\n",
      "9977                                                   []\n",
      "9978                                   [thankyou, shilpa]\n",
      "9979                                          [get, issu]\n",
      "9980                                              [gener]\n",
      "9981                              [khanda, coloni, phone]\n",
      "9982                                               [nice]\n",
      "9983                                        [give, copun]\n",
      "9984                                    [hay, good, morn]\n",
      "9985                                         [bank, loan]\n",
      "9986                                               [year]\n",
      "9987                     [work, ..i, m, first, tme, user]\n",
      "9988                                                   []\n",
      "9989                                      [given, answer]\n",
      "9990                           [provid, warang, code, na]\n",
      "9991    [aur, phir, mera, munh, bund, kardo, apn, khoo...\n",
      "9992                                 [good, even, anjali]\n",
      "9993                                              [readi]\n",
      "9994                                               [fast]\n",
      "9995                                             [ctedit]\n",
      "9996                           [forst, transact, n, fail]\n",
      "9997                                               [vika]\n",
      "9998               [u, give, 5, pic, rakul, preet, singh]\n",
      "9999                                    [bb, vine, dekht]\n",
      "Name: message, Length: 10000, dtype: object\n",
      "'preprocess'  19091.31 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "X_train shape: (40659,)\n",
       "y_train shape: (40659, 9)\n",
       "X_test shape: (10000,)\n",
       "y_test shape: (10000, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.preprocess(stop_lists=[sw_curated, temporal, misc, pleasantries, address, hindi])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'vectorize'  2320.87 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "X_train shape: (40659,)\n",
       "y_train shape: (40659, 9)\n",
       "X_test shape: (10000,)\n",
       "y_test shape: (10000, 9)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.vectorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", 600)\n",
    "# x.wordfreq.to_csv(\"wordfreq.csv\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'reduce_dimensions'  69.63 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "X_train shape: (40659,)\n",
       "y_train shape: (40659, 9)\n",
       "X_test shape: (10000,)\n",
       "y_test shape: (10000, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reduce_dimensions(k=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'classify'  157714.21 ms\n",
      "accuracy_label:  0.945844444444\n",
      "accuracy_subset:  0.734\n",
      "classification report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.81      0.60      0.69       810\n",
      "          1       0.77      0.70      0.74       450\n",
      "          2       0.69      0.29      0.41       371\n",
      "          3       0.87      0.84      0.85       893\n",
      "          4       0.80      0.85      0.83      2138\n",
      "          5       0.69      0.61      0.65       807\n",
      "          6       0.75      0.62      0.68       637\n",
      "          7       0.83      0.84      0.84      3282\n",
      "          8       0.92      0.51      0.65      1668\n",
      "\n",
      "avg / total       0.82      0.72      0.76     11056\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "X_train shape: (40659,)\n",
       "y_train shape: (40659, 9)\n",
       "X_test shape: (10000,)\n",
       "y_test shape: (10000, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.classify(model=RandomForestClassifier(n_estimators=200, max_depth=2500,\n",
    "                                        min_samples_split=5, n_jobs=-1,\n",
    "                                        max_features=0.008, # 0.00735215 = sqrt(18,500)/18,500\n",
    "                                        random_state=42, verbose=0)).results()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:greyatom]",
   "language": "python",
   "name": "conda-env-greyatom-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
